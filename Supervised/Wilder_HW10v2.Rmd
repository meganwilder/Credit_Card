---
title: "APAN5420 --- HW 10, Credit Card Transactions"
author: 'Megan Wilder'
date: "8/3/18"
output:
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 1
  html_document: 
    df_print: default
    number_sections: yes
    toc: no
    toc_depth: 1
---

-----

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


```{r, include=FALSE}
#load libraries
library(kableExtra)
library(knitr)
library(caret)
library(pROC)
library(dplyr)
library(h2o)
library(ROSE)
library(ggplot2)

#load data
ccard <- read.csv("creditcard.csv")

#explore dataframe
dim(ccard)
summary(ccard)
lapply(ccard, class)
#view table of class variable
table(ccard$Class)
#view percentage of fraud transactions
prop.table(table(ccard$Class)) * 100  #0.173%, imbalanced dataset

#graph Class variable
ggplot(data = ccard, aes(x = Class)) + geom_bar()

#columns overview
#Time - (numeric) Number of seconds elapsed between this transaction and
#the first transaction in the dataset
#V1 through V28 - (numeric) Features V1 through V28 are the principal components
#obtained with PCA
#Amount - (numeric) Transaction amount
#Class - (integer) 1 for fraudulent transactions, 0 otherwise

#change Class variable to categorical from numerical to categorical variable
ccard$Class <- factor(ccard$Class)

```

#Down-Sampling for the Majority Class
```{r}
#split dataset into training and test sets
#set seed
set.seed(123)

# Sample into 3 sets. 60% train, 20% validation and 20% test
idx <-
sample(
seq(1, 3),
size = nrow(ccard),
replace = TRUE,
prob = c(.6, .2, .2)
)
train <- ccard[idx == 1,]
test <- ccard[idx == 2,]
val <- ccard[idx == 3,]

#check classes distribution
kable(prop.table(table(train$Class)))
kable(prop.table(table(test$Class)))
kable(prop.table(table(val$Class)))

#down-sampling, sample so that fraud represents about 10% of data set
#a typical range for resampling is to make fraud 5-20% of the training set.
#want to make it a significant amount of the training set but not
#amplyify the noise too much.
data_balanced_under <-
ovun.sample(
Class ~ .,
data = train,
method = "under",
p = 0.1,
seed = 1
)$data

#view table of class variable in rebalanced training set
kable(table(data_balanced_under$Class))
#view classes distribution in rebalanced training set
kable(prop.table(table(data_balanced_under$Class)))

```

```{r, results="hide"}
#Start H2O
h2o.init(nthreads = -1, max_mem_size = '8G')

# clean slate in case the cluster was already running
h2o.removeAll()

```

#GBM Base Model, Without Tuning Hyperparameters 
A gradient boosting machine computes a sequence of weak learners (typically very simple trees), where each successive tree is built for the prediction residuals of the preceding tree. It is an ensemble method, which combines several base models to produce one optimal predictive model. The combined estimator is usually better than any of the single base estimators as its bias is reduced.  

```{r}
# make h2o data.frame, loads into H2O service
train.hex <- as.h2o(data_balanced_under)
test.hex <- as.h2o(test)
val.hex <- as.h2o(val)

# Response and predictors to use
resp <- "Class"
pred <- setdiff(names(train.hex), 'Class')

# Build a baseline gbm model without hyperparameter tuning
gbm <- h2o.gbm(x = pred,
y = resp,
training_frame = train.hex)
gbm

# Get the AUC on the validation set
h2o.auc(h2o.performance(gbm, newdata = val.hex)) #0.9875728

```

#Tune Hyperparameters with Grid Search
Hyperparameters:    
Learning rate (shrinkage): A value between 0 and 1, corresponds to how quickly the error is corrected from each tree to the next. A lower learning rate is generally better, but will require more trees (and computational time). A large learning rate makes the system unable to settle down.  
Learn rate annealing: hyperparameter to decay the learning rate. Speeds up convergence without sacrificing too much accuracy. 
Max Depth: The maximum allowed depth for the trees. Deeper trees take longer to train.  
Sample Rate: Row sampling rate, can improve generalization and lead to lower validation and test set errors. Rule of thumb for large datasets is around 0.7 to 0.8 (sampling 70-80% of the data).  
Column Sample Rate: Column sampling rate, can improve generalization and lead to lower validation and test set errors. Rule of thumb for large datasets is around 0.7 to 0.8 (sampling 70-80% of the data).  
Number of trees: # of trees used    
score_tree_interval = 10: Score every 10 trees to make early stopping reproducible.  
max_runtime_secs=1200: Early stopping based on timeout. In this case no more than 1200 seconds.  
stopping_rounds = 5,  
stopping_tolerance = 1e-4,  
stopping_metric = "AUC",   
The above three hyperparameters control the early stopping when the AUC does not improve by at least 0.01% for 5 consecutive scoring events.

Grid Search: I used H2o's grid search to train and validate numerous models at once based on different hyper-parameter levels.   

(Source: https://blog.h2o.ai/2016/06/h2o-gbm-tuning-tutorial-for-r/)

```{r, results="hide"}
#create list of hyperparameters to tune
hyper_params <- list(
learn_rate = c(0.01, .05, 0.1),
max_depth = seq(2, 12, 2),
sample_rate = c(0.7, 0.8, 1.0),
col_sample_rate = c(0.7, 0.8, 1.0),
ntrees = seq(500, 2000, 500)
)

#Cartesian Grid Search
grid <- h2o.grid(
hyper_params = hyper_params,
search_criteria = list(strategy = "Cartesian"),
algorithm = "gbm",
grid_id = "gbm_grid",
x = pred,
y = resp,
training_frame = train.hex,
validation_frame = val.hex,
seed = 123,
learn_rate_annealing = .99,
max_runtime_secs = 1200,
#Early stopping based on timeout. In this case no more than 1200 seconds.
stopping_rounds = 5,
stopping_tolerance = 1e-4,
stopping_metric = "AUC",
#The above three hyperparameters control the early stopping when 
#the AUC does not improve by at least 0.01% for 5 consecutive 
#scoring events.
score_tree_interval = 10 #Score every 10 trees to make early stopping reproducible.
)
```

```{r}
#view grid
grid

## sort the grid models by decreasing AUC
sortedGrid <-
h2o.getGrid("gbm_grid", sort_by = "auc", decreasing = TRUE)
print(sortedGrid)

#print AUC for 10 best models
for (i in 1:10) {
topModels <- h2o.getModel(sortedGrid@model_ids[[i]])
print(h2o.auc(h2o.performance(topModels, valid = TRUE)))
} #best model had AUC of 0.9909801 on validation set

#name model with highest AUC best model
best_model <-
h2o.getModel(sortedGrid@model_ids[[1]]) 
#better than my base model, which had an AUC of 0.9875728 on 
#its validation set

#best model's parameters
best_model@parameters

#view best model
summary(best_model)

#get the actual number of trees
ntrees <- best_model@parameters[["ntrees"]]
ntrees #500

#get the actual max depth
mdepth <- best_model@parameters[["max_depth"]]
mdepth #10

#Validation set used to select the best model
#Evaluate the model performance on test set to get honest estimate of model performance
best_model_perf <- h2o.performance(model = best_model,
newdata = test.hex)

#model performance metrics on test set
best_model_perf

#MSE
h2o.mse(best_model_perf) #0.00233282, not really relevant for classification problems
#RMSE
h2o.rmse(best_model_perf) #0.04829927, not really relevant for classification problems
#Log Loss
h2o.logloss(best_model_perf) #0.0112636
#AUC
h2o.auc(best_model_perf)  #0.9631125, slighly less than the AUC on the validation set
#Gini
h2o.giniCoef(best_model_perf) #0.9262249

#best model performance metrics at all thresholds
test.scores <- best_model_perf@metrics$thresholds_and_metric_scores

#find best threshold that maximizes F1
best.thresh <- test.scores$threshold[which.max(test.scores$f1)]
best.thresh #0.9780643

#create dataframe with performance metrics of model on test data at
#threshold that maximizes F1
metrics <- data_frame(
Precision = h2o.precision(best_model_perf, best.thresh),
Recall = h2o.recall(best_model_perf, best.thresh),
F1 = h2o.F1(best_model_perf, best.thresh),
AUC = h2o.auc(best_model_perf),
LogLoss = h2o.logloss(best_model_perf),
Gini = h2o.giniCoef(best_model_perf),
Accuracy = h2o.accuracy(best_model_perf, best.thresh),
Mean_Accuracy = h2o.mean_per_class_accuracy(best_model_perf, best.thresh)
)

#view metrics
kable(metrics) %>%
kable_styling(bootstrap_options = "striped", full_width = F)
#overall it appears that my model performed well

```

#Plot ROC, Precision-recall
```{r}
#ROC
tpr = as.data.frame(h2o.tpr(best_model_perf))
fpr = as.data.frame(h2o.fpr(best_model_perf))
ROC_out <- merge(tpr, fpr, by = 'threshold')
head(ROC_out)

#Plot ROC
ggplot(ROC_out, aes(x = fpr, y = tpr)) +
theme_bw() +
geom_line() +
ggtitle("ROC") + ylab("TPR") + xlab("FPR")
#ROC curves plot the tradeoff between recall and false positive rates

#Precision Recall
#Evaluate the predictive performance of model based on precision and recall
head(h2o.F1(best_model_perf))
precision = as.data.frame(h2o.precision(best_model_perf))
recall = as.data.frame(h2o.recall(best_model_perf))
PR_out <- merge(precision, recall, by = 'threshold')
head(PR_out)

#Plot Precision - Recall
ggplot(PR_out, aes(x = tpr, y = precision)) +
theme_bw() +
geom_line() +
ggtitle("Precision-Recall") + ylab("Precision") + xlab("Recall")
#Precision-recall curves shows the tradeoff between precision and recall
#for different thresholds. Useful measure of prediction success when
#modeling rare events (classes very imbalanced).
#High precision relates to a low false positive rate,
#and high recall relates to a low false negative rate. High scores
#for both show that the classifier is returning accurate results
#(high precision), as well as returning a majority of all
#positive results (high recall).
#(source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)

#plot threshold that maximizes F1
ggplot(test.scores, aes(x = threshold, y = f1)) +
geom_line() +
geom_point() +
geom_vline(xintercept = best.thresh,
linetype = "dashed",
color = "red") +
labs(x = "Prob. Cutoff", y = "F1-Measure")

```

  
#Variable Importance & Partial Dependence Plot
```{r, fig.width=10,fig.height=11}
#variable importance
h2o.varimp(best_model)
#plot variable importance
h2o.varimp_plot(best_model)

#partial dependence
h2o.partialPlot(
object = best_model,
data = train.hex,
plot = TRUE,
plot_stddev = FALSE
)
#Partial dependence plot gives a graphical depiction of 
#the marginal effect of a variable on the response. 
#The effect of a variable is measured in change in the mean response.

# All done. Shut down H2O.
#h2o.shutdown(prompt = FALSE)
```
Variable Importance:
In the variable importance graph, the top variables contribute more to the model than the bottom ones. Here, V14 and V10 are the two most significant variables. Too many variables can result in overfitting.  In order to mitigate overfitting I could reduce the number of variables used to only the most important variables.

Partial Dependence:
Partial dependence plots gives a graphical depiction of the marginal effect of a variable on the response (fraud in our case). The effect of a variable is measured in change in the mean response.
